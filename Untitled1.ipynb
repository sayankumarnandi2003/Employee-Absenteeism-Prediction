{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53172b1-48cb-4d70-bb0f-adbef8a676d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model.py\n",
    "# Run this script in the same folder as your project.\n",
    "# It will create new 'model' and 'scaler' files.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"Starting model retraining process...\")\n",
    "\n",
    "# --- 1. Load The Original Training Data ---\n",
    "# IMPORTANT: You need the original CSV file used for training.\n",
    "# For this example, I'm using a common public version of this dataset.\n",
    "# Please REPLACE 'Absenteeism_at_work.csv' with the name of your actual training data file.\n",
    "try:\n",
    "    # Attempt to load a common version of this dataset.\n",
    "    # You can download it from the UCI Machine Learning Repository if you don't have it.\n",
    "    raw_df = pd.read_csv('Absenteeism_at_work.csv', sep=';')\n",
    "    print(\"Successfully loaded 'Absenteeism_at_work.csv'.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n---\")\n",
    "    print(\"ERROR: Training data file not found.\")\n",
    "    print(\"Please download 'Absenteeism_at_work.csv' from the UCI repository or use your original training file.\")\n",
    "    print(\"Stopping script.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- 2. Preprocessing the Data ---\n",
    "# This section mimics the preprocessing from your module to ensure consistency.\n",
    "print(\"Preprocessing data...\")\n",
    "\n",
    "# Create the target variable: 1 for excessive absenteeism, 0 for moderate.\n",
    "# We use the median as a cutoff point, which is a common practice for this dataset.\n",
    "median_absenteeism = raw_df['Absenteeism time in hours'].median()\n",
    "targets = np.where(raw_df['Absenteeism time in hours'] > median_absenteeism, 1, 0)\n",
    "raw_df['Excessive_Absenteeism'] = targets\n",
    "\n",
    "# Drop the original absenteeism column\n",
    "df = raw_df.drop(['Absenteeism time in hours'], axis=1)\n",
    "\n",
    "# Create dummy variables for reasons for absence\n",
    "reason_columns = pd.get_dummies(df['Reason for Absence'], drop_first=True)\n",
    "reason_type_1 = reason_columns.loc[:, 1:14].max(axis=1)\n",
    "reason_type_2 = reason_columns.loc[:, 15:17].max(axis=1)\n",
    "reason_type_3 = reason_columns.loc[:, 18:21].max(axis=1)\n",
    "reason_type_4 = reason_columns.loc[:, 22:].max(axis=1)\n",
    "\n",
    "# Drop the original reason column and concatenate the new dummy variables\n",
    "df = df.drop(['Reason for Absence'], axis=1)\n",
    "df = pd.concat([df, reason_type_1, reason_type_2, reason_type_3, reason_type_4], axis=1)\n",
    "\n",
    "# Rename the new reason columns\n",
    "df.columns = [\n",
    "    'ID', 'Date', 'Transportation expense', 'Distance from Residence to Work', 'Service time', 'Age',\n",
    "    'Work load Average/day ', 'Hit target', 'Disciplinary failure', 'Education', 'Son',\n",
    "    'Social drinker', 'Social smoker', 'Pet', 'Weight', 'Height', 'Body mass index',\n",
    "    'Excessive_Absenteeism', 'Reason_1', 'Reason_2', 'Reason_3', 'Reason_4'\n",
    "]\n",
    "\n",
    "# Convert 'Date' and extract Month and Day of the Week\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n",
    "df['Month Value'] = df['Date'].apply(lambda x: x.month)\n",
    "df['Day of the Week'] = df['Date'].apply(lambda x: x.weekday())\n",
    "df = df.drop(['Date'], axis=1)\n",
    "\n",
    "# Map Education into fewer categories\n",
    "df['Education'] = df['Education'].map({1: 0, 2: 1, 3: 1, 4: 1})\n",
    "\n",
    "# Drop columns that are not needed for the model as per your module\n",
    "df = df.drop(['ID', 'Service time', 'Work load Average/day ', 'Distance from Residence to Work', 'Day of the Week'], axis=1)\n",
    "\n",
    "# Reorder columns to be safe\n",
    "df = df[[\n",
    "    'Reason_1', 'Reason_2', 'Reason_3', 'Reason_4', 'Month Value',\n",
    "    'Transportation expense', 'Age', 'Hit target', 'Disciplinary failure',\n",
    "    'Education', 'Son', 'Social drinker', 'Social smoker', 'Pet',\n",
    "    'Weight', 'Height', 'Body mass index', 'Excessive_Absenteeism'\n",
    "]]\n",
    "\n",
    "print(\"Preprocessing complete.\")\n",
    "\n",
    "# --- 3. Define Features (Inputs) and Target ---\n",
    "unscaled_inputs = df.drop('Excessive_Absenteeism', axis=1)\n",
    "targets = df['Excessive_Absenteeism']\n",
    "\n",
    "\n",
    "# --- 4. Scale the Data ---\n",
    "print(\"Scaling features...\")\n",
    "\n",
    "# We will scale only a subset of the columns, similar to your original project.\n",
    "# The CustomScaler in your module was designed to do this, here we do it manually.\n",
    "columns_to_scale = [\n",
    "    'Month Value', 'Transportation expense', 'Age', 'Hit target',\n",
    "    'Son', 'Pet', 'Weight', 'Height', 'Body mass index'\n",
    "]\n",
    "# Create the scaler\n",
    "scaler = StandardScaler()\n",
    "# Fit the scaler ONLY on the columns that need scaling\n",
    "scaler.fit(unscaled_inputs[columns_to_scale])\n",
    "# Transform the columns and create a new DataFrame\n",
    "scaled_columns = scaler.transform(unscaled_inputs[columns_to_scale])\n",
    "scaled_inputs_df = pd.DataFrame(scaled_columns, columns=columns_to_scale)\n",
    "\n",
    "# Get the columns that were not scaled\n",
    "unscaled_columns = unscaled_inputs.drop(columns_to_scale, axis=1)\n",
    "\n",
    "# Concatenate the unscaled and scaled columns to create the final inputs\n",
    "# We reset index to prevent issues with joining\n",
    "inputs = pd.concat([unscaled_columns.reset_index(drop=True), scaled_inputs_df], axis=1)\n",
    "\n",
    "print(\"Scaling complete.\")\n",
    "\n",
    "# --- 5. Train the Model ---\n",
    "print(\"Splitting data and training the model...\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(inputs, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "model = LogisticRegression(solver='liblinear') # Using a compatible solver\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Check model accuracy on the test set\n",
    "accuracy = model.score(x_test, y_test)\n",
    "print(f\"Model training complete. Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# --- 6. Save the New Model and Scaler ---\n",
    "print(\"Saving new 'model' and 'scaler' files...\")\n",
    "\n",
    "with open('model', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "\n",
    "with open('scaler', 'wb') as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)\n",
    "\n",
    "print(\"\\nâœ… Success! New 'model' and 'scaler' files have been created.\")\n",
    "print(\"You can now re-run your original Jupyter Notebook cell.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
